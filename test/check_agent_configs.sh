#! /bin/bash

set -e

if [ -z "$1" ]; then
  echo "Usage: $0 <helm-export-file>"
  exit 1
fi

if ! command -v yq &> /dev/null; then
  echo "yq could not be found; please install form the latest release here: https://github.com/mikefarah/yq"
  exit 1
fi

if ! command -v observe-agent &> /dev/null; then
  echo "observe-agent could not be found; please install from the latest release https://docs.observeinc.com/en/latest/content/observe-agent/linux_install.html"
  exit 1
fi

tmp_dir="/tmp/observe-agent-helm-check"
rm -rf $tmp_dir
mkdir $tmp_dir

# Relies on https://github.com/mikefarah/yq for yaml parsing
if ! yq -e 'select(.kind == "ConfigMap" and .metadata.name == "observe-agent").data.relay' $1 > $tmp_dir/observe-agent.yaml; then
  echo "Failed to find observe-agent ConfigMaps; is $1 a kubernetes manifest YAML file generated by the observe-agent helm chart?"
  exit 1
fi

any_failed=false

confs=$(yq -e 'select(.kind == "ConfigMap").metadata.name' $1 | grep -Ev 'cluster-name|observe-agent|---')
for conf in $confs; do
    echo "Checking $conf..."
    fname="$tmp_dir/$conf.yaml"
    yq -e "select(.kind == \"ConfigMap\" and .metadata.name == \"$conf\").data.relay" "$1" > "$fname"
    if [ ! -f "$fname" ]; then
        echo "File $fname not found!" 1>&2
        any_failed=true
        continue
    fi

    # TODO eventually we should migrate this script to running a job in kubernetes and doing full validation rather than hacking the config to get it to run on a host.
    yq_commands=(
      # Remove any eks cloud detector since it errors outside of kubernetes
      '(.processors."resourcedetection/cloud".detectors) = ["ec2"]'

      # Remove root_path from hostmetrics since it errors outside of linux
      'del(.receivers.hostmetrics.root_path)'

      # Remove references to bearer_token_file for prom scraping since they may not be present
      'del(.receivers.prometheus*.config.scrape_configs[].bearer_token_file)'

      # Remove kubeletstats since it errors outside of kubernetes
      'del(.receivers.kubeletstats*)'
      '(.service.pipelines.*.receivers[] | select(. == "kubeletstats*")) |= "nop"'

      # Remove loadbalancing exporter since it errors outside of kubernetes
      'del(.exporters.loadbalancing*)'
      '(.service.pipelines.*.exporters[] | select(. == "loadbalancing*")) |= "nop"'
    )
    for ((i = 0; i < ${#yq_commands[@]}; i++)); do
      yq_cmd="${yq_commands[$i]}"
      if ! yq -i "$yq_cmd" $fname; then
        echo "failed to apply YQ modification: $yq_cmd" 1>&2
        any_failed=true
        break
      fi
    done

    # Set various env vars to match what's provided in our helm chart pod definitions.
    if ! env MY_POD_IP=0.0.0.0 \
        GOMEMLIMIT=409MiB \
        OBSERVE_CLUSTER_NAME=observe-agent-monitored-cluster \
        OBSERVE_CLUSTER_UID=abc123 \
        OTEL_K8S_POD_UID=abc456 \
        OTEL_K8S_POD_NAME=test-pod \
        K8S_NODE_NAME=test-node \
        KUBERNETES_SERVICE_HOST=0.0.0.0 \
        KUBERNETES_SERVICE_PORT=1234 \
        TOKEN=1234567890abcdefghij:1234567890abcdefghijklmnopqrstuv \
        TRACES_TOKEN=1234567890abcdefghij:1234567890abcdefghijklmnopqrstuv \
        observe-agent --config-mode=docker \
        --observe-config=$tmp_dir/observe-agent.yaml \
        --config=$tmp_dir/$conf.yaml config validate;
    then
        any_failed=true
    fi
    echo ""
done

if [ "$any_failed" = true ]; then
  echo
  echo "---"
  echo "⚠️  One or more agent configs failed validation!"
  echo
  exit 1
fi
