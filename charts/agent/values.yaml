# This section is for configuring agent to send data to Observe endpoints
observe:
  # If create = false it is assumed that a secret named agent-credentials with key: OBSERVE_TOKEN already exists
  token:
    create: false
    value: ""
  collectionEndpoint:
    # ex -  https://12345678.collect.observeinc.com
    value: ""
  # this is temporary and will be removed
  entityToken:
    # To create secret
    create: false
    value: ""
    # To use exporter
    use: false
  traceToken:
    create: false
    value: ""

cluster:
  # -- name given to your cluster.
  # popuplates to k8s.cluster.name per https://opentelemetry.io/docs/specs/semconv/resource/k8s/#cluster
  name: observe-agent-monitored-cluster
  deploymentEnvironment:
    # -- deployment environment cluster runs in (e.g. testing, production, etc).
    # populates to deployment.environment.name per https://opentelemetry.io/docs/specs/semconv/resource/deployment-environment/
    name: ""
  # configure the collection of resources and events
  events:
    # how often to pull resources from cluster
    pullInterval: 20m
    enabled: true
  # cluster-level metrics and entity events (as metrics) from the Kubernetes API server. It uses the K8s API to listen for updates.
  metrics:
    enabled: true
    interval: 60s
  # namespace to use/create
  namespaceOverride:
    # !!! IMPORTANT !!! This needs to have same value as namespaceOverride in deployments and daemonsets below
    value: observe
  # typically the clusterUid is set to kube-system namespace uid - if you need a different value use this override
  uidOverride:
    value: ""

node:
  # -- Enables the node-logs-metrics agent daemonset for collection of node logs and metrics.
  # The nodes on which metrics and logs are collected can be configured via `affinity` in the `node-logs-metrics` section below.
  # This should be set to false to disable the node-log-metrics daemonset when running in a serverless environment (ex: EKS Fargate).
  enabled: true
  metrics:
    enabled: true
    interval: 60s
    fileSystem:
      rootPath: /hostfs
      excludeMountPoints: '["/dev/*","/proc/*","/sys/*","/run/k3s/containerd/*","/var/lib/docker/*","/var/lib/kubelet/*","/snap/*"]'
    cadvisor:
      enabled: false
  containers:
    logs:
      enabled: true
      # log lines above this size will be truncated
      maxLogSize: 512kb
      # If true, the receiver will pause reading a file and attempt to resend the current batch of logs if it encounters an error from downstream components.
      retryOnFailure:
        enabled: true
        # Time to wait after the first failure before retrying.
        initialInterval: 1s
        # Upper bound on retry backoff interval. Once this value is reached the delay between consecutive retries will remain constant at the specified value.
        maxInterval: 30s
        # Maximum amount of time (including retries) spent trying to send a logs batch to a downstream consumer. Once this value is reached, the data is discarded. Retrying never stops if set to 0.
        maxElapsedTime: 5m
      # A list of file glob patterns that match the file paths to be read. Need to put inside of '' to avoid helm stripping commas and quotes.
      # Log rotation glob assumes rename of file follows this pattern *.log.*
      include: '["/var/log/pods/*/*/*.log", "/var/log/pods/*/*/*.log.*", "/var/log/kube-apiserver-audit.log"]'
      # A list of file glob patterns to exclude from reading. This is applied against the paths matched by include. Need to put inside of '' to avoid helm stripping commas and quotes.
      #  Log rotation exclude glob assumes files previously rotated and renamed are zipped after rotated file is created
      exclude: '["**/*.gz", "**/*.tmp"]'
      # time unit 1m, 1h
      lookbackPeriod: 24h
      # At startup, where to start reading logs from the file. Options are beginning or end.
      startAt: end
      # -- Multiline config block. Cannot be combined with automatic multiline detection.
      # For more information see https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/filelogreceiver/README.md#multiline-configuration
      multiline:
      # -- Enable automatic detection of multiline logs that start with a timestamp. Cannot be combined with multiline config block.
      # Examples of supported timestamp formats:
      # `2025-03-28 13:45:30`,
      # `2025-03-28T14:33:53.743350Z`,
      # `Jul 15 15:16:01`,
      # `2025/05/16 19:46:15`
      autoMultilineDetection: false
    # pulls node, pod, and container metrics from the API server on a kubelet and sends it down the metric pipeline for further processing.
    metrics:
      enabled: true
      interval: 60s
  kubeletstats:
    # Explicitly toggles between K8S_NODE_IP and K8S_NODE_NAME. When set to false, it uses the default value of K8S_NODE_NAME from open-telemetry/opentelemetry-collector-contrib.
    # this resolves issues similar to https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/26481#issuecomment-1720797914 for `no such host` or `connection refused`.
    useNodeIp: false
  forwarder:
    enabled: true
    traces:
      enabled: true
      # -- (string) The max span duration to be considered by the agent, or "none" for no limit. Any span over this limit will be dropped. Durations must be a number with a valid time unit: https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/ottl/ottlfuncs/README.md#duration
      maxSpanDuration: 1h
    metrics:
      enabled: true
      # -- (string) The format of the outbound metrics from the forwarder to Observe. Valid values are "prometheus" and "otel"
      outputFormat: "prometheus"
      # -- (bool) Converts cumulative metrics to delta metrics; all OTel metrics should be sent to Observe with temporality delta for the best experience.
      convertCumulativeToDelta: false
    logs:
      enabled: true

application:
  # use this option to scrape prometheus metrics from pods
  # To enable/disable auto discovery of metrics - besides option below - you can annotate your pods
  # See helm-charts/examples/agent/pod_metrics for more information
  prometheusScrape:
    enabled: false
    # add another dedicated deployment instead of scraping from cluster-metrics service
    independentDeployment: false
    # scrape interval
    interval: 60s
    # namespaces to exclude from scraping
    # drop is processed first so all namespaces that match regex will be dropped
    namespaceDropRegex: (.*istio.*|.*ingress.*|kube-system)
    # namespaces to explicity include for scraping - can use or (ns1|ns2)
    # keep is processed after drop so only remaining namespaces that match regex will be kept
    namespaceKeepRegex: (.*)
    # port names to scrape from - can use or .*metrics|otherportname
    portKeepRegex: .*metrics
    # metrics to drop
    metricDropRegex: ""
    # metrics to keep
    metricKeepRegex: (.*)
  REDMetrics:
    # -- (bool) Whether to enable generating RED metrics from spans. See https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/connector/spanmetricsconnector#overview
    enabled: false
    # -- (bool) If enabled, this will skip generating RED metrics for spans that are not service entrypoint spans (which are spans with kind Server, kind Consumer, or calls to DB or messaging system clients).
    # If this and trace sampling are both enabled, it will not be possible to deduce accurate RED metrics for span types other than service entrypoint spans.
    onlyGenerateForServiceEntrypointSpans: false
    # -- (list) List of resource attributes to include as dimensions for RED metrics. See https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/connector/spanmetricsconnector#overview
    resourceDimensions:
      - service.namespace
      - service.version
      - deployment.environment
      - k8s.pod.name
      - k8s.namespace.name
    # -- (list) List of span attributes to include as dimensions for RED metrics. See https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/connector/spanmetricsconnector#overview
    spanDimensions:
      - peer.db.name
      - peer.messaging.system
      - otel.status_description
      - observe.status_code

nodeless:
  # -- (bool) Enables nodeless mode. Nodeless mode is intended for environments where daemonsets are not supported.
  enabled: false
  # -- (string) The hosting platform for the nodeless mode. Valid values are "fargate".
  hostingPlatform: ""
  metrics:
    enabled: false
  # -- (object) A map of namespaces to lists of service accounts. If you provide service accounts here
  # we will attach a cluster role and binding granting the service accounts permission to the relevant Kubernetes APIs
  # needed to collect metrics. If empty, you will need to manually grant the service accounts the necessary permissions.
  # Example:
  #   serviceAccounts:
  #     default: ["app1-sa", "app2-sa"]
  #     fargate-ns: ["fargate-app-sa"]
  serviceAccounts: {}

gatewayDeployment:
  # -- (bool) Whether to enable the gateway deployment. The gateway is responsible for sampling and traces before sending them to Observe.
  enabled: false
  traceSampling:
    # -- (bool) Whether to enable trace sampling. Sampling will only occur if the gateway deployment is enabled. See https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/tailsamplingprocessor#tail-sampling-processor
    enabled: true
    # -- (object) Full configuration for the tail sampling processor. See https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/tailsamplingprocessor
    config:
      # -- (list) List of sampling policies to apply. See https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/tailsamplingprocessor#a-practical-example
      policies:
        # Send all spans with status_code error to Observe
        - name: retain-all-errors
          type: status_code
          status_code: {status_codes: [ERROR]}
        # Send 10% of all other spans to Observe
        - name: sample-probabilistically-10-percent
          type: probabilistic
          probabilistic: {sampling_percentage: 10}

agent:
  config:
    global:
      processors:
        batch:
          sendBatchSize: 4096
          sendBatchMaxSize: 4096
          timeout: 5s
        k8sattributesNodeOnlyDaemonset: false
      exporters:
        sendingQueue:
          enabled: true
          batch:
            enabled: false
            flushTimeout: 5s
            sizer: bytes
            minSize: 0
            maxSize: 41943040  # 40MiB
        retryOnFailure:
          enabled: true
          # Time to wait after the first failure before retrying.
          initialInterval: 1s
          # Upper bound on retry backoff interval. Once this value is reached the delay between consecutive retries will remain constant at the specified value.
          maxInterval: 30s
          # Maximum amount of time (including retries) spent trying to send a logs batch to a downstream consumer. Once this value is reached, the data is discarded. Retrying never stops if set to 0.
          maxElapsedTime: 5m
        timeout: 10s
      service:
        telemetry:
          metricsLevel: normal
          loggingLevel: WARN
          loggingEncoding: console
      debug:
        enabled: false
        # values basic, normal, detailed
        verbosity: basic
      fleet:
        enabled: false
        interval: 10m
        configInterval: 24h
      # -- Additional OTel collector config for all agent deployments/daemonsets
      overrides:

    # -- Additional OTel collector config for cluster-events deployment
    clusterEvents:
    # Put any OTel config overrides here.

    # -- Additional OTel collector config for cluster-metrics deployment
    clusterMetrics:
    # Put any OTel config overrides here.

    # -- Additional OTel collector config for prometheus-scraper deployment
    prometheusScraper:
    # Put any OTel config overrides here.

    # -- Additional OTel collector config for node-logs-metrics daemonset
    nodeLogsMetrics:
    # Put any OTel config overrides here. For example, to adjust the metrics that are collected via kubeletstats receiver, you can do the following:
    # receivers:
    #   kubeletstats:
    #     metric_groups:
    #       - container
    #       - node
    #       - pod
    #       - volume

    # -- Additional OTel collector config for monitor deployment
    monitor:
    # Put any OTel config overrides here.

    # -- Additional OTel collector config for forwarder daemonset
    forwarder:
    # Put any OTel config overrides here. For example, to forward spans to an additional OTel http endpoint:
    # exporters:
    #   otlphttp/extra:
    #     endpoint: http://my-otel-endpoint:4317
    #     headers:
    #       authorization: "Bearer my-token"
    # service:
    #   pipelines:
    #     traces/observe-forward:
    #       exporters:
    #         - otlphttp/extra
    #         - otlphttp/observe/forward/trace

    # -- Additional OTel collector config for gateway deployment
    gateway:
    # Put any OTel config overrides here.

  selfMonitor:
    enabled: true
    metrics:
      scrapeInterval: 60s


################################################
cluster-events:
  mode: deployment

  # ----------------------------------------- #
  # Different for each deployment/daemonset #
  nameOverride: "cluster-events"
  # !!! IMPORTANT !!! This needs to have same value as namespaceOverride in cluster above
  namespaceOverride: "observe"

  configMap:
    create: false
    existingName: "cluster-events"

  resources:
    requests:
      cpu: 150m
      memory: 256Mi
    limits:
      memory: 256Mi
  # ----------------------------------------- #

  # ----------------------------------------- #
  # Same for each deployment/daemonset      #
  image:
    repository: observeinc/observe-agent
    tag: 2.10.1
    pullPolicy: IfNotPresent

  command:
    name: "observe-agent"
    extraArgs: ["start", "--observe-config=/observe-agent-conf/observe-agent.yaml", "--config=/conf/relay.yaml"]

  serviceAccount:
    create: false
    name: "observe-agent-service-account"

  clusterRole:
    create: false
    name: "observe-agent-cluster-role"

  livenessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5
  readinessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5

  networkPolicy:
    enabled: true
    egressRules: [{}]

  podAnnotations: {
    # This stops optional prometheus scrape config from picking up these pods
    observeinc_com_scrape: 'false',
    observe_monitor_purpose: observecollection,
    observe_monitor_scrape: 'true',
    observe_monitor_path: '/metrics',
    observe_monitor_port: '8888',
  }

  # Standard anti-affinity rules will exclude any node labeled with observeinc.com/unschedulable or using windows os
  # See helm-charts/examples/agent/affinity/README.md for more information
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: observeinc.com/unschedulable
                operator: DoesNotExist
              - key: kubernetes.io/os
                operator: NotIn
                values: [windows]

  tolerations: []

  ports:
    metrics:
      # The metrics port is disabled by default. However you need to enable the port
      # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
  # this init container provides the cluster uid (kube-system namespace) as config map
  initContainers:
    - name: kube-cluster-info
      image: observeinc/kube-cluster-info:v0.11.5
      imagePullPolicy: Always
      env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
  # extract clusteruid from configmap create by init container
  extraEnvs:
    - name: OTEL_K8S_POD_UID
      valueFrom:
        fieldRef:
          fieldPath: metadata.uid
    - name: OTEL_K8S_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: OBSERVE_CLUSTER_NAME
      valueFrom:
        configMapKeyRef:
          name: cluster-name
          key: name
    - name: OBSERVE_CLUSTER_UID
      valueFrom:
        configMapKeyRef:
          name: cluster-info
          key: id
    - name: TOKEN
      valueFrom:
        secretKeyRef:
          name: agent-credentials
          key: OBSERVE_TOKEN
          optional: true
  extraEnvsFrom: []
  extraVolumes:
    - name: "observe-agent-deployment-config"
      configMap:
        name: "observe-agent"
        items:
          - key: "relay"
            path: "observe-agent.yaml"
        defaultMode: 420

  extraVolumeMounts:
    - name: observe-agent-deployment-config
      mountPath: /observe-agent-conf
  # ----------------------------------------- #

################################################
cluster-metrics:
  mode: deployment
  # ----------------------------------------- #
  # Different for each deployment/daemonset #
  nameOverride: "cluster-metrics"
  # !!! IMPORTANT !!! This needs to have same value as namespaceOverride in cluster above
  namespaceOverride: "observe"

  configMap:
    create: false
    existingName: "cluster-metrics"

  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      memory: 512Mi
  # ----------------------------------------- #

  # ----------------------------------------- #
  # Same for each deployment/daemonset      #
  image:
    repository: observeinc/observe-agent
    tag: 2.10.1
    pullPolicy: IfNotPresent

  command:
    name: "observe-agent"
    extraArgs: ["start", "--observe-config=/observe-agent-conf/observe-agent.yaml", "--config=/conf/relay.yaml", "--feature-gates=+exporter.prometheusremotewritexporter.EnableMultipleWorkers"]

  serviceAccount:
    create: false
    name: "observe-agent-service-account"
  clusterRole:
    create: false
    name: "observe-agent-cluster-role"

  livenessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5

  readinessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5

  networkPolicy:
    enabled: true
    egressRules: [{}]

  podAnnotations: {
    # This stops optional prometheus scrape config from picking up these pods
    observeinc_com_scrape: 'false',
    observe_monitor_purpose: observecollection,
    observe_monitor_scrape: 'true',
    observe_monitor_path: '/metrics',
    observe_monitor_port: '8888',
  }

  # Standard anti-affinity rules will exclude any node labeled with observeinc.com/unschedulable or using windows os
  # See helm-charts/examples/agent/affinity/README.md for more information
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: observeinc.com/unschedulable
                operator: DoesNotExist
              - key: kubernetes.io/os
                operator: NotIn
                values: [windows]

  tolerations: []

  ports:
    metrics:
      # The metrics port is disabled by default. However you need to enable the port
      # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
  # this init container provides the cluster uid (kube-system namespace) as config map
  initContainers:
    - name: kube-cluster-info
      image: observeinc/kube-cluster-info:v0.11.5
      imagePullPolicy: Always
      env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
  # extract clusteruid from configmap create by init container
  extraEnvs:
    - name: OTEL_K8S_POD_UID
      valueFrom:
        fieldRef:
          fieldPath: metadata.uid
    - name: OTEL_K8S_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: OBSERVE_CLUSTER_NAME
      valueFrom:
        configMapKeyRef:
          name: cluster-name
          key: name
    - name: OBSERVE_CLUSTER_UID
      valueFrom:
        configMapKeyRef:
          name: cluster-info
          key: id
    - name: TOKEN
      valueFrom:
        secretKeyRef:
          name: agent-credentials
          key: OBSERVE_TOKEN
          optional: true
  extraEnvsFrom: []
  extraVolumes:
    - name: "observe-agent-deployment-config"
      configMap:
        name: "observe-agent"
        items:
          - key: "relay"
            path: "observe-agent.yaml"
        defaultMode: 420
  extraVolumeMounts:
    - name: observe-agent-deployment-config
      mountPath: /observe-agent-conf
  # ----------------------------------------- #

################################################
prometheus-scraper:
  mode: deployment
  # ----------------------------------------- #
  # Different for each deployment/daemonset #
  nameOverride: "prometheus-scraper"
  # !!! IMPORTANT !!! This needs to have same value as namespaceOverride in cluster above
  namespaceOverride: "observe"

  configMap:
    create: false
    existingName: "prometheus-scraper"

  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      memory: 512Mi
  # ----------------------------------------- #

  # ----------------------------------------- #
  # Same for each deployment/daemonset      #
  image:
    repository: observeinc/observe-agent
    tag: 2.10.1
    pullPolicy: IfNotPresent

  command:
    name: "observe-agent"
    extraArgs: ["start", "--observe-config=/observe-agent-conf/observe-agent.yaml", "--config=/conf/relay.yaml", "--feature-gates=+exporter.prometheusremotewritexporter.EnableMultipleWorkers"]

  serviceAccount:
    create: false
    name: "observe-agent-service-account"
  clusterRole:
    create: false
    name: "observe-agent-cluster-role"

  livenessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5

  readinessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5

  networkPolicy:
    enabled: true
    egressRules: [{}]

  podAnnotations: {
    # This stops optional prometheus scrape config from picking up these pods
    observeinc_com_scrape: 'false',
    observe_monitor_purpose: observecollection,
    observe_monitor_scrape: 'true',
    observe_monitor_path: '/metrics',
    observe_monitor_port: '8888',
  }

  # Standard anti-affinity rules will exclude any node labeled with observeinc.com/unschedulable or using windows os
  # See helm-charts/examples/agent/affinity/README.md for more information
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: observeinc.com/unschedulable
                operator: DoesNotExist
              - key: kubernetes.io/os
                operator: NotIn
                values: [windows]

  tolerations: []

  ports:
    metrics:
      # The metrics port is disabled by default. However you need to enable the port
      # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
  # this init container provides the cluster uid (kube-system namespace) as config map
  initContainers:
    - name: kube-cluster-info
      image: observeinc/kube-cluster-info:v0.11.5
      imagePullPolicy: Always
      env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
  # extract clusteruid from configmap create by init container
  extraEnvs:
    - name: OTEL_K8S_POD_UID
      valueFrom:
        fieldRef:
          fieldPath: metadata.uid
    - name: OTEL_K8S_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: OBSERVE_CLUSTER_NAME
      valueFrom:
        configMapKeyRef:
          name: cluster-name
          key: name
    - name: OBSERVE_CLUSTER_UID
      valueFrom:
        configMapKeyRef:
          name: cluster-info
          key: id
    - name: TOKEN
      valueFrom:
        secretKeyRef:
          name: agent-credentials
          key: OBSERVE_TOKEN
          optional: true
  extraEnvsFrom: []
  extraVolumes:
    - name: "observe-agent-deployment-config"
      configMap:
        name: "observe-agent"
        items:
          - key: "relay"
            path: "observe-agent.yaml"
        defaultMode: 420
  extraVolumeMounts:
    - name: observe-agent-deployment-config
      mountPath: /observe-agent-conf
  # ----------------------------------------- #

################################################
node-logs-metrics:
  mode: daemonset
  # ----------------------------------------- #
  # Different for each deployment/daemonset #
  nameOverride: "node-logs-metrics"
  # !!! IMPORTANT !!! This needs to have same value as namespaceOverride in cluster above
  namespaceOverride: "observe"

  configMap:
    create: false
    existingName: "node-logs-metrics"

  resources:
    requests:
      cpu: 250m
      memory: 512Mi
    limits:
      memory: 512Mi
  # ----------------------------------------- #

  # ----------------------------------------- #
  # Same for each deployment/daemonset      #
  image:
    repository: observeinc/observe-agent
    tag: 2.10.1
    pullPolicy: IfNotPresent

  command:
    name: "observe-agent"
    extraArgs: ["start", "--observe-config=/observe-agent-conf/observe-agent.yaml", "--config=/conf/relay.yaml", "--feature-gates=+exporter.prometheusremotewritexporter.EnableMultipleWorkers"]

  serviceAccount:
    create: false
    name: "observe-agent-service-account"
  clusterRole:
    create: false
    name: "observe-agent-cluster-role"

  livenessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5

  readinessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5

  podAnnotations: {
    # This stops optional prometheus scrape config from picking up these pods
    observeinc_com_scrape: 'false',
    observe_monitor_purpose: observecollection,
    observe_monitor_scrape: 'true',
    observe_monitor_path: '/metrics',
    observe_monitor_port: '8888',
  }

  # Standard anti-affinity rules will exclude any node labeled with observeinc.com/unschedulable or using windows os
  # See helm-charts/examples/agent/affinity/README.md for more information
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: observeinc.com/unschedulable
                operator: DoesNotExist
              - key: kubernetes.io/os
                operator: NotIn
                values: [windows]

  tolerations: []

  ports:
    otlp:
      enabled: false
    otlp-http:
      enabled: false
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      # The metrics port is disabled by default. However you need to enable the port
      # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
  # this init container provides the cluster uid (kube-system namespace) as config map
  initContainers:
    - name: kube-cluster-info
      image: observeinc/kube-cluster-info:v0.11.5
      imagePullPolicy: Always
      env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
  # extract clusteruid from configmap create by init container
  extraEnvs:
    - name: OTEL_K8S_POD_UID
      valueFrom:
        fieldRef:
          fieldPath: metadata.uid
    - name: OTEL_K8S_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: OBSERVE_CLUSTER_NAME
      valueFrom:
        configMapKeyRef:
          name: cluster-name
          key: name
    - name: OBSERVE_CLUSTER_UID
      valueFrom:
        configMapKeyRef:
          name: cluster-info
          key: id
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: K8S_NODE_IP
      valueFrom:
        fieldRef:
          fieldPath: status.hostIP
    - name: TOKEN
      valueFrom:
        secretKeyRef:
          name: agent-credentials
          key: OBSERVE_TOKEN
          optional: true
    - name: TRACES_TOKEN
      valueFrom:
        secretKeyRef:
          name: agent-credentials
          key: TRACES_TOKEN
          optional: true
  extraEnvsFrom: []
  extraVolumes:
    - name: "observe-agent-deployment-config"
      configMap:
        name: "observe-agent"
        items:
          - key: "relay"
            path: "observe-agent.yaml"
        defaultMode: 420
    - name: varlogpods
      hostPath:
        path: /var/log/pods
    - name: varlibdockercontainers
      hostPath:
        path: /var/lib/docker/containers
    - name: varlibotelcol
      hostPath:
        path: /var/lib/otelcol
        type: DirectoryOrCreate
    - name: hostfs
      hostPath:
        path: /
  extraVolumeMounts:
    - name: observe-agent-deployment-config
      mountPath: /observe-agent-conf
    - name: varlogpods
      mountPath: /var/log/pods
      readOnly: true
    - name: varlibdockercontainers
      mountPath: /var/lib/docker/containers
      readOnly: true
    - name: varlibotelcol
      mountPath: /var/lib/otelcol
    - name: hostfs
      mountPath: /hostfs
      readOnly: true
      mountPropagation: HostToContainer

  securityContext:
    runAsUser: 0
    runAsGroup: 0
  # ----------------------------------------- #

################################################
monitor:
  mode: deployment
  # ----------------------------------------- #
  # Different for each deployment/daemonset #
  nameOverride: "monitor"
  # !!! IMPORTANT !!! This needs to have same value as namespaceOverride in cluster above
  namespaceOverride: "observe"

  configMap:
    create: false
    existingName: "monitor"

  resources:
    requests:
      cpu: 150m
      memory: 256Mi
    limits:
      memory: 256Mi
  # ----------------------------------------- #

  # ----------------------------------------- #
  # Same for each deployment/daemonset      #
  image:
    repository: observeinc/observe-agent
    tag: 2.10.1
    pullPolicy: IfNotPresent

  command:
    name: "observe-agent"
    extraArgs: ["start", "--observe-config=/observe-agent-conf/observe-agent.yaml", "--config=/conf/relay.yaml", "--feature-gates=+exporter.prometheusremotewritexporter.EnableMultipleWorkers"]

  serviceAccount:
    create: false
    name: "observe-agent-service-account"
  clusterRole:
    create: false
    name: "observe-agent-cluster-role"

  livenessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5

  readinessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5

  networkPolicy:
    enabled: true
    egressRules: [{}]

  podAnnotations: {
    # This stops optional prometheus scrape config from picking up these pods
    observeinc_com_scrape: 'false',
    observe_monitor_purpose: observecollection,
    observe_monitor_scrape: 'false',
    observe_monitor_path: '/metrics',
    observe_monitor_port: '8888',
  }

  # Standard anti-affinity rules will exclude any node labeled with observeinc.com/unschedulable or using windows os
  # See helm-charts/examples/agent/affinity/README.md for more information
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: observeinc.com/unschedulable
                operator: DoesNotExist
              - key: kubernetes.io/os
                operator: NotIn
                values: [windows]

  tolerations: []

  ports:
    metrics:
      # The metrics port is disabled by default. However you need to enable the port
      # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
  # this init container provides the cluster uid (kube-system namespace) as config map
  initContainers:
    - name: kube-cluster-info
      image: observeinc/kube-cluster-info:v0.11.5
      imagePullPolicy: Always
      env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
  # extract clusteruid from configmap create by init container
  extraEnvs:
    - name: OTEL_K8S_POD_UID
      valueFrom:
        fieldRef:
          fieldPath: metadata.uid
    - name: OTEL_K8S_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: OBSERVE_CLUSTER_NAME
      valueFrom:
        configMapKeyRef:
          name: cluster-name
          key: name
    - name: OBSERVE_CLUSTER_UID
      valueFrom:
        configMapKeyRef:
          name: cluster-info
          key: id
    - name: TOKEN
      valueFrom:
        secretKeyRef:
          name: agent-credentials
          key: OBSERVE_TOKEN
          optional: true
  extraEnvsFrom: []
  extraVolumes:
    - name: "observe-agent-deployment-config"
      configMap:
        name: "observe-agent"
        items:
          - key: "relay"
            path: "observe-agent.yaml"
        defaultMode: 420
  extraVolumeMounts:
    - name: observe-agent-deployment-config
      mountPath: /observe-agent-conf
  # ----------------------------------------- #

forwarder:
  # -- The forwarder is run as a daemonset by default, but can be run as a deployment by setting mode to "deployment". Deployment mode
  # must be used when running in a serverless environment (ex: EKS Fargate) where daemonsets are not supported.
  mode: daemonset

  # -- The `replicaCount` is only used when `mode` is set to "deployment". It is ignored when `mode` is set to "daemonset".
  # In deployment mode, this sets the number of replicas (ie the number of forwarder pods to run).
  replicaCount: 1

  # ----------------------------------------- #
  # Different for each deployment/daemonset #
  nameOverride: "forwarder"
  # !!! IMPORTANT !!! This needs to have same value as namespaceOverride in cluster above
  namespaceOverride: "observe"

  configMap:
    create: false
    existingName: "forwarder"

  resources:
    requests:
      cpu: 300m
      memory: 512Mi
    limits:
      memory: 512Mi
  # ----------------------------------------- #

  # ----------------------------------------- #
  # Same for each deployment/daemonset      #
  image:
    repository: observeinc/observe-agent
    tag: 2.10.1
    pullPolicy: IfNotPresent

  command:
    name: "observe-agent"
    extraArgs: ["start", "--observe-config=/observe-agent-conf/observe-agent.yaml", "--config=/conf/relay.yaml"]

  serviceAccount:
    create: false
    name: "observe-agent-service-account"
  clusterRole:
    create: false
    name: "observe-agent-cluster-role"

  livenessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5

  readinessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5

  service:
    enabled: true
    type: ClusterIP

  networkPolicy:
    enabled: true
    egressRules: [{}]

  podAnnotations: {
    # This stops optional prometheus scrape config from picking up these pods
    observeinc_com_scrape: 'false',
    observe_monitor_purpose: observecollection,
    observe_monitor_scrape: 'true',
    observe_monitor_path: '/metrics',
    observe_monitor_port: '8888',
  }

  # Standard anti-affinity rules will exclude any node labeled with observeinc.com/unschedulable or using windows os
  # See helm-charts/examples/agent/affinity/README.md for more information
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: observeinc.com/unschedulable
                operator: DoesNotExist
              - key: kubernetes.io/os
                operator: NotIn
                values: [windows]

  tolerations: []

  ports:
    metrics:
      # The metrics port is disabled by default. However you need to enable the port
      # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
  # this init container provides the cluster uid (kube-system namespace) as config map
  initContainers:
    - name: kube-cluster-info
      image: observeinc/kube-cluster-info:v0.11.5
      imagePullPolicy: Always
      env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
  # extract clusteruid from configmap create by init container
  extraEnvs:
    - name: OTEL_K8S_POD_UID
      valueFrom:
        fieldRef:
          fieldPath: metadata.uid
    - name: OTEL_K8S_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: OBSERVE_CLUSTER_NAME
      valueFrom:
        configMapKeyRef:
          name: cluster-name
          key: name
    - name: OBSERVE_CLUSTER_UID
      valueFrom:
        configMapKeyRef:
          name: cluster-info
          key: id
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: TOKEN
      valueFrom:
        secretKeyRef:
          name: agent-credentials
          key: OBSERVE_TOKEN
          optional: true
    - name: TRACE_TOKEN
      valueFrom:
        secretKeyRef:
          name: agent-credentials
          key: TRACE_TOKEN
          optional: true
  extraEnvsFrom: []
  extraVolumes:
    - name: "observe-agent-deployment-config"
      configMap:
        name: "observe-agent"
        items:
          - key: "relay"
            path: "observe-agent.yaml"
        defaultMode: 420
  extraVolumeMounts:
    - name: observe-agent-deployment-config
      mountPath: /observe-agent-conf
  # ----------------------------------------- #

gateway:
  mode: deployment

  # -- (int) The number of pods to use for the gateway deployment.
  replicaCount: 3

  # ----------------------------------------- #
  # Different for each deployment/daemonset #
  nameOverride: "gateway"
  # !!! IMPORTANT !!! This needs to have same value as namespaceOverride in cluster above
  namespaceOverride: "observe"

  configMap:
    create: false
    existingName: "gateway"

  resources:
    requests:
      cpu: 300m
      memory: 768Mi
    limits:
      memory: 768Mi
  # ----------------------------------------- #

  # ----------------------------------------- #
  # Same for each deployment/daemonset      #
  image:
    repository: observeinc/observe-agent
    tag: 2.10.1
    pullPolicy: IfNotPresent

  command:
    name: "observe-agent"
    extraArgs: ["start", "--observe-config=/observe-agent-conf/observe-agent.yaml", "--config=/conf/relay.yaml"]

  serviceAccount:
    create: false
    name: "observe-agent-service-account"
  clusterRole:
    create: false
    name: "observe-agent-cluster-role"

  livenessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5

  readinessProbe:
    httpGet:
      port: 13133
      path: /status
    initialDelaySeconds: 30
    periodSeconds: 5

  service:
    enabled: true
    type: ClusterIP

  networkPolicy:
    enabled: true
    egressRules: [{}]
    allowIngressFrom:
      - podSelector:
          matchLabels:
            app.kubernetes.io/name: forwarder
            app.kubernetes.io/instance: observe-agent

  podAnnotations: {
    # This stops optional prometheus scrape config from picking up these pods
    observeinc_com_scrape: 'false',
    observe_monitor_purpose: observecollection,
    observe_monitor_scrape: 'true',
    observe_monitor_path: '/metrics',
    observe_monitor_port: '8888',
  }

  # Standard anti-affinity rules will exclude any node labeled with observeinc.com/unschedulable or using windows os
  # See helm-charts/examples/agent/affinity/README.md for more information
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: observeinc.com/unschedulable
                operator: DoesNotExist
              - key: kubernetes.io/os
                operator: NotIn
                values: [windows]

  tolerations: []

  ports:
    metrics:
      # The metrics port is disabled by default. However you need to enable the port
      # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
  # this init container provides the cluster uid (kube-system namespace) as config map
  initContainers:
    - name: kube-cluster-info
      image: observeinc/kube-cluster-info:v0.11.5
      imagePullPolicy: Always
      env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
  # extract clusteruid from configmap create by init container
  extraEnvs:
    - name: OTEL_K8S_POD_UID
      valueFrom:
        fieldRef:
          fieldPath: metadata.uid
    - name: OTEL_K8S_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: OBSERVE_CLUSTER_NAME
      valueFrom:
        configMapKeyRef:
          name: cluster-name
          key: name
    - name: OBSERVE_CLUSTER_UID
      valueFrom:
        configMapKeyRef:
          name: cluster-info
          key: id
    - name: K8S_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: TOKEN
      valueFrom:
        secretKeyRef:
          name: agent-credentials
          key: OBSERVE_TOKEN
          optional: true
  extraEnvsFrom: []
  extraVolumes:
    - name: "observe-agent-deployment-config"
      configMap:
        name: "observe-agent"
        items:
          - key: "relay"
            path: "observe-agent.yaml"
        defaultMode: 420
  extraVolumeMounts:
    - name: observe-agent-deployment-config
      mountPath: /observe-agent-conf
  # ----------------------------------------- #
